# ğŸ“š Agentic RAG Local â€” Privacy-First AI Knowledge Assistant

A fully local Retrieval-Augmented Generation system that can read and answer questions using **your PDFs only** â€” no cloud, no data leak, 100% offline.

---

## âœ¨ Features

- âœ… Upload and index PDFs
- âœ… Local LLM inference with Ollama (Phi-3-tiny)
- âœ… Semantic vector search via Qdrant
- âœ… Accurate answers with document citations
- âœ… Multiple conversation memory
- âœ… Modern UI with Streamlit

---

## ğŸ§© Tech Stack

| Component | Role |
|----------|------|
| Python | App logic |
| Streamlit | User interface |
| SentenceTransformers | Embeddings |
| Qdrant (Docker) | Vector storage & search |
| Ollama + Phi-3-tiny | Local LLM |

---

## âœ… Requirements

| Requirement | Minimum |
|------------|--------|
| Python | 3.10+ |
| RAM | 8 GB |
| Disk | 5 GB free |
| Docker | Installed & running |
| Ollama | Installed |

---

## ğŸš€ Installation

```bash
# Clone the repository
git clone https://github.com/azizrebhi/RAG_AGENT.git
cd RAG_AGENT

# Create & activate venv
python -m venv venv
.\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create environment file
copy .env.example .env
```

### ğŸ³ Start Qdrant

```bash
docker run -d --name qdrant \
  -p 6333:6333 -p 6334:6334 \
  qdrant/qdrant:latest
```
Dashboard: http://localhost:6333/dashboard

### ğŸ¤– Install Local Model

```bash
ollama pull phi3:tiny
```

### â–¶ï¸ Run the App

```bash
streamlit run app.py
```

## ğŸ“ How to Use

1. Upload PDFs in the sidebar
2. Click "Ingest PDFs"
3. Ask questions in the chat box
4. View answers + cited document chunks

### Example Questions

```sql
What is Value-at-Risk?
Explain Monte-Carlo simulation in risk management.
```

## ğŸ§± Architecture

```
Streamlit UI â†’ RAG Core â†’ Qdrant (search)
                       â†’ Ollama (LLM generation)
âœ… Everything stays local
```

## ğŸ“ Project Structure

```
rag-agentic-local/
â”œâ”€ app.py
â”œâ”€ rag_core.py
â”œâ”€ ingest.py
â”œâ”€ memory.py
â”œâ”€ models.py
|
â”œâ”€ uploads/           # temporary PDF storage (ignored in git)
â”œâ”€ conversations/     # conversation memory (ignored in git)
|
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ .env.example
```

## â— Troubleshooting

| Issue | Fix |
|-------|-----|
| LLM fails to load | Use smaller model: phi3:tiny |
| No results | Ensure PDFs ingested successfully |
| Qdrant 404 | Start Docker Qdrant container |
| Unicode issues | UTF-8 fixed in subprocess handling |

## ğŸ”® Roadmap

- [ ] Better citation ranking
- [ ] PDF viewer with highlighted answers
- [ ] Full Docker Compose deployment
- [ ] Conversation summarization
- [ ] RAG with web deep-research mode

## ğŸ† Credits

- [Qdrant](https://qdrant.tech/) â€” Vector Database
- [Ollama](https://ollama.ai/) â€” Local LLM runtime
- [Streamlit](https://streamlit.io/) â€” UI Framework
- [SentenceTransformers](https://www.sbert.net/) â€” Embedding model

Built by Maison Info ğŸ§ âš¡  
Guided by Agentic AI Engineering ğŸ¤